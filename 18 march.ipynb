{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods \\n   measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared \\n   to wrapper methods as they do not involve training the models.\\nThere are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods\\n(ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree).\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods \n",
    "   measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared \n",
    "   to wrapper methods as they do not involve training the models.\n",
    "There are three types of feature selection: Wrapper methods (forward, backward, and stepwise selection), Filter methods\n",
    "(ANOVA, Pearson correlation, variance thresholding), and Embedded methods (Lasso, Ridge, Decision Tree).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance\\n  of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature \\n  by actually training a model on it.\\n  \\n  It assesses the quality of learning with different subsets of features against the evaluation criterion, and the output\\n  would be the model's performance versus different sets of features. Finally, the user can select the optimum set of features\\n  for which the model's performance is optimum.\\n  \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance\n",
    "  of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature \n",
    "  by actually training a model on it.\n",
    "  \n",
    "  It assesses the quality of learning with different subsets of features against the evaluation criterion, and the output\n",
    "  would be the model's performance versus different sets of features. Finally, the user can select the optimum set of features\n",
    "  for which the model's performance is optimum.\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some examples of embedded methods include decision tree-based algorithms (e.g., decision tree, random forest, gradient \\n   boosting), and feature selection using regularization models (e.g., LASSO or elastic net).\\n   \\n  Fisher score is one of the most widely used supervised feature selection methods. The algorithm we will use returns the ranks\\n  of the variables based on the fisher's score in descending order. We can then select the variables as per the case.\\n \\n Advantages of Embedded Methods:\\n1. They take into consideration the interaction of features like wrapper methods do.\\n2. They are faster like filter methods.\\n3. They are more accurate than filter methods.\\n4. They find the feature subset for the algorithm being trained.\\n5. They are much less prone to over-fitting.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Some examples of embedded methods include decision tree-based algorithms (e.g., decision tree, random forest, gradient \n",
    "   boosting), and feature selection using regularization models (e.g., LASSO or elastic net).\n",
    "   \n",
    "  Fisher score is one of the most widely used supervised feature selection methods. The algorithm we will use returns the ranks\n",
    "  of the variables based on the fisher's score in descending order. We can then select the variables as per the case.\n",
    " \n",
    " Advantages of Embedded Methods:\n",
    "1. They take into consideration the interaction of features like wrapper methods do.\n",
    "2. They are faster like filter methods.\n",
    "3. They are more accurate than filter methods.\n",
    "4. They find the feature subset for the algorithm being trained.\n",
    "5. They are much less prone to over-fitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is\\n   considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold \\n   point for rankings to select only the required features and exclude noise.\\n  \\n  Disadvantages of Filtration\\n1. Filters can only work on liquids and gasses.\\n2. Autoclaving is usually cheaper than filtration since filters are expensive to replace, especially nano-filters.\\n3. Glass filters are very brittle and can break easily.\\n4. Membrane filters rupture easily.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature is\n",
    "   considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold \n",
    "   point for rankings to select only the required features and exclude noise.\n",
    "  \n",
    "  Disadvantages of Filtration\n",
    "1. Filters can only work on liquids and gasses.\n",
    "2. Autoclaving is usually cheaper than filtration since filters are expensive to replace, especially nano-filters.\n",
    "3. Glass filters are very brittle and can break easily.\n",
    "4. Membrane filters rupture easily.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand,\\n   wrapper methods are computationally very expensive as well. Filter methods use statistical methods for evaluation of a \\n   subset of features while wrapper methods use cross validation.\\n   \\n   Reducing the complexity of a model and making it easier to interpret. Building a sensible model with better prediction \\n   power. Reducing over-fitting by selecting the right set of features.\\n   \\n   Filter method is faster and useful when there are more number of features. Wrapper method gives better performance while\\n   the embedded method lies in between the other two methods.\\n   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand,\n",
    "   wrapper methods are computationally very expensive as well. Filter methods use statistical methods for evaluation of a \n",
    "   subset of features while wrapper methods use cross validation.\n",
    "   \n",
    "   Reducing the complexity of a model and making it easier to interpret. Building a sensible model with better prediction \n",
    "   power. Reducing over-fitting by selecting the right set of features.\n",
    "   \n",
    "   Filter method is faster and useful when there are more number of features. Wrapper method gives better performance while\n",
    "   the embedded method lies in between the other two methods.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First get the feature importance of each feature of your dataset by using the feature importance property of the model. \\n   Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is \\n   the feature towards your output variable.\\n   \\n   Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods\\n   measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared \\n   to wrapper methods as they do not involve training the models.\\n   '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''First get the feature importance of each feature of your dataset by using the feature importance property of the model. \n",
    "   Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is \n",
    "   the feature towards your output variable.\n",
    "   \n",
    "   Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods\n",
    "   measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared \n",
    "   to wrapper methods as they do not involve training the models.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Embedded methods combine the qualities' of filter and wrapper methods. It's implemented by algorithms that have their own\\n   built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which \\n   have inbuilt penalization functions to reduce overfitting.\\n   \\n   You can get the feature importance of each feature of your dataset by using the feature importance property of the model. \\n   Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the\\n   feature towards your output variable.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Embedded methods combine the qualities' of filter and wrapper methods. It's implemented by algorithms that have their own\n",
    "   built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which \n",
    "   have inbuilt penalization functions to reduce overfitting.\n",
    "   \n",
    "   You can get the feature importance of each feature of your dataset by using the feature importance property of the model. \n",
    "   Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the\n",
    "   feature towards your output variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to\\n   fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against \\n   the evaluation criterion.\\n   Example: For regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification \\n   the evaluation criterion can be accuracy, precision, recall, f1-score, etc.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to\n",
    "   fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against \n",
    "   the evaluation criterion.\n",
    "   Example: For regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification \n",
    "   the evaluation criterion can be accuracy, precision, recall, f1-score, etc.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
